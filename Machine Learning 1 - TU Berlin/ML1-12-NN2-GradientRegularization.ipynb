{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks 2\n",
    "\n",
    "In this homework, we will train neural networks on the Breast Cancer dataset. For this, we will use of the Pytorch library. We will also make use of scikit-learn for the ML baselines. A first part of the homework will analyze the parameters of the network before and after training. A second part of the homework will test some regularization penalties and their effect on the generalization error.\n",
    "\n",
    "## Breast Cancer Dataset\n",
    "\n",
    "The following code extracts the Breast cancer dataset in a way that is already partitioned into training and test data. The data is normalized such that each dimension has mean 0 and variance 1. To test the robustness of our learning models, we also artificially inject 4% of mislabelings in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\hugue\\anaconda3\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hugue\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "Xtrain,Ttrain,Xtest,Ttest = utils.breast_cancer()\n",
    "\n",
    "nx = Xtrain.shape[1]   # Numpy.shape() gives us the number of elements in each direction of an array\n",
    "nh = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 \n",
      "\n",
      "[[-0.16069934  0.30396373 -0.09000201 ...  1.3787293   1.1867002\n",
      "   1.50716413]\n",
      " [-0.21569922  1.36371081 -0.27983205 ... -0.76994762 -0.76348354\n",
      "  -0.89700039]\n",
      " [ 0.01877396 -0.7849096  -0.05027987 ... -0.44242438 -1.2899135\n",
      "  -0.94059054]\n",
      " ...\n",
      " [-1.06096057  1.89246411 -1.05044162 ... -0.30747473 -1.13608656\n",
      "  -0.06152249]\n",
      " [-0.01596281  0.01718227 -0.08623886 ... -0.45220119 -0.9138921\n",
      "  -0.44153919]\n",
      " [-0.64990882 -0.83195968 -0.65907398 ... -0.49456738 -0.21483412\n",
      "  -0.32809303]]\n"
     ]
    }
   ],
   "source": [
    "print(nx, \"\\n\") # == 30 : donc 30 élément \n",
    "print(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Classifier\n",
    "\n",
    "In this homework, we consider the same architecture as the one considered in Exercise 2 of the theoretical part. The class `NeuralNetworkClassifier` implements this network. The function `reg` is a regularizer which we set initially to zero (i.e. no regularizer). Because the dataset is small, the network can be optimized in batch mode, using the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy,torch,sklearn,sklearn.metrics\n",
    "from torch import nn,optim\n",
    "\n",
    "class NeuralNetworkClassifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        torch.manual_seed(0)\n",
    "        \n",
    "        self.model = nn.Sequential(nn.Linear(nx,nh),nn.ReLU()) \n",
    "            # https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential\n",
    "        with torch.no_grad(): list(self.model)[0].weight *= 0.1  \n",
    "            # The \"with\" statement ensure proper acquisition and release of the ressource\n",
    "            # The wrapper with torch.no_grad() temporarily sets all of the requires_grad flags to false\n",
    "        self.s = torch.zeros([100]); self.s[:50] = 1; self.s[50:] = -1  \n",
    "            # s will be the vector of labels for an estimation s appartient {-1,1}, see xercice 2\n",
    "        self.pool  = lambda y: y.matmul(self.s)\n",
    "        self.loss  = lambda y,t: torch.clamp(1-y*t,min=0).mean() \n",
    "\n",
    "    def reg(self): return 0\n",
    "        \n",
    "    def fit(self,X,T,nbit=10000):\n",
    "        \n",
    "        X = torch.Tensor(X)\n",
    "        T = torch.Tensor(T)\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(),lr=0.01)\n",
    "        for _ in range(nbit):\n",
    "            optimizer.zero_grad()\n",
    "            (self.loss(self.pool(self.model(X)),T)+self.reg()).backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "    def predict(self,X):\n",
    "        return self.pool(self.model(torch.Tensor(X)))\n",
    "\n",
    "    def score(self,X,T):\n",
    "        Y = numpy.sign(self.predict(X).data.numpy())\n",
    "        return sklearn.metrics.accuracy_score(T,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Performance vs. Baselines\n",
    "\n",
    "We compare the performance of the neural network on the Breast cancer data to two other classifiers: a random forest and a support vector classification model with RBF kernel. We use the scikit-learn implementation of these models, with their default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble,svm\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier(random_state=0)\n",
    "rfc.fit(Xtrain,Ttrain)\n",
    "\n",
    "svc = svm.SVC()\n",
    "svc.fit(Xtrain,Ttrain)\n",
    "\n",
    "nnc = NeuralNetworkClassifier()\n",
    "nnc.fit(Xtrain,Ttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">    RForest | Train Acc:  1.000 | Test Acc:  0.940\n",
      ">        SVC | Train Acc:  0.958 | Test Acc:  0.951\n",
      ">         NN | Train Acc:  1.000 | Test Acc:  0.884\n"
     ]
    }
   ],
   "source": [
    "def pretty(name,model):\n",
    "    return '> %10s | Train Acc: %6.3f | Test Acc: %6.3f'%(name,model.score(Xtrain,Ttrain),model.score(Xtest,Ttest))\n",
    "\n",
    "print(pretty('RForest',rfc))\n",
    "print(pretty('SVC',svc))\n",
    "print(pretty('NN',nnc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network performs not as good as the baselines. Most likely, the neural network has overfitted its decision boundary, in particular, on the mislabeled training examples.\n",
    "\n",
    "## Gradient, and Parameter Norms :\n",
    "\n",
    "For the model to generalize better, we assume that the gradient of the decision function should be prevented from becoming too large. Because the gradient can only be evaluated on the current data distribution (and may not generalize outside the data), we resort to the following inequality we have proven in the theoretical section for this class of neural network models:\n",
    "\n",
    "$$\n",
    "\\text{Grad} \\leq \\|W\\|_\\text{Mix} \\leq \\sqrt{h}\\|W\\|_\\text{Frob}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $\\|W\\|_\\text{Frob} =  \\sqrt{\\sum_{i=1}^d \\sum_{j=1}^h  w_{ij}^2}$\n",
    "* $\\|W\\|_\\text{Mix} = \\sqrt{\\sum_{i=1}^d \\big(\\sum_{j=1}^h | w_{ij}|\\big)^2}$\n",
    "* $\\text{Grad} = \\textstyle \\frac1N \\sum_{n=1}^N\\|\\nabla_{\\boldsymbol{x}}f (\\boldsymbol{x_n})\\|$\n",
    "\n",
    "and where $d$ is the number of input features, $h$ is the number of neurons in the hidden layer, and $W$ is the matrix of weights in the first layer (*Note that in PyTorch, the matrix of weights is given in transposed form*).\n",
    "\n",
    "As a first step, we would like to keep track of these quantities during training. The function `Frob(nn)` that computes $\\|W\\|_\\text{Frob}$ is already implemented for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Frob(nn):\n",
    "    W = list(nn.model)[0].weight\n",
    "    return (W**2).sum()**.5\n",
    "    \n",
    "def Mix(nn):\n",
    "    W = list(nn.model)[0].weight\n",
    "    return (W.abs().sum(dim=0)**2).sum()**.5\n",
    "    \n",
    "def Grad(nn,X):\n",
    "    X = torch.Tensor(X)\n",
    "    X.requires_grad_(True)\n",
    "    nn.predict(X).sum().backward() \n",
    "    return ((X.grad**2).sum(dim=1)**.5).mean() # Norm ... Somme des carrées puis ^0.5\n",
    "    # With the backward call what we will get as a result will be a size number of data point = nb of dimension, and pushes the gradient back to th einput and store itin a variable X.grad\n",
    "    # To store the gradienf of the input X.requires_grad_(True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=30, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(nn.Sequential(nn.Linear(nx,nh),nn.ReLU()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code measures these three quantities before and after training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">     Before | Train Acc:  0.391 | Test Acc:  0.372 | Grad:   0.389 | WMix:   4.966 | sqrt(h)*WFrob:   5.751\n",
      ">      After | Train Acc:  1.000 | Test Acc:  0.884 | Grad:   7.297 | WMix:  40.103 | sqrt(h)*WFrob:  56.739\n"
     ]
    }
   ],
   "source": [
    "def fullpretty(name,nn):\n",
    "    return pretty(name,nn) + ' | Grad: %7.3f | WMix: %7.3f | sqrt(h)*WFrob: %7.3f'%(Grad(nn,Xtest),Mix(nn),nh**.5*Frob(nn))\n",
    "\n",
    "nnr = NeuralNetworkClassifier()\n",
    "print(fullpretty('Before',nnr))\n",
    "nnr.fit(Xtrain,Ttrain)\n",
    "print(fullpretty('After',nnr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the inequality $\\text{Grad} \\leq \\|W\\|_\\text{Mix} \\leq \\sqrt{h} \\|W\\|_\\text{Frob}$ we have proven also holds empirically. We also observe that these quantities tend to increase as training proceeds. This is a typical behavior, as the network starts rather simple and becomes complex as more and more variations in the training data are being captured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norm Penalties :\n",
    "\n",
    "We consider the new objective $J^\\text{Frob}(\\theta) = \\text{MSE}(\\theta) + \\lambda \\cdot (\\sqrt{h} \\|W\\|_\\text{Frob})^2$, where the first term is the original mean square error objective and where the second term is the added penalty. We hardcode the penalty coeffecient to $\\lambda = 0.005$. In principle, for maximum performance and fair comparison between the methods, several of them should be tried (also for other models), and selected based on some validation set. Here, for simplicity, we omit this step.\n",
    "\n",
    "A downside of the Frobenius norm is that it is not a very tight upper bound of the gradient, that is, penalizing it is does not penalize specifically high gradient. Instead, other useful properties of the model could be negatively affected by it. Therefore, we also experiment with the mixed-norm regularizer $\\textstyle \\lambda \\cdot \\|W\\|_\\text{Mix}^2$, which is a tighter bound of the gradient, and where we also hardcode the penalty coefficient to $\\lambda = 0.025$.\n",
    "\n",
    "\n",
    "Two new functions for regularization function : \n",
    "\n",
    "- Frobenius norm regularizer\n",
    "- Mixed norm regularizer\n",
    "* Create two new classifiers by reimplementing the regularization function with the Frobenius norm regularizer and Mixed norm regularizer respectively. You may for this task call the norm functions implemented in the question above, but this time you also need to ensure that these functions can be differentiated w.r.t. the weight parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below implements and train neural networks with the new regularizers, and compares the performance with the previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrobClassifier(NeuralNetworkClassifier):\n",
    "    \n",
    "    def reg(self):\n",
    "        h = len(self.s)\n",
    "        # Needs to be recovered from the model, no access to nn directly\n",
    "        return 0.005 * h * Frob(self)**2\n",
    "    \n",
    "class MixClassifier(NeuralNetworkClassifier):\n",
    "    \n",
    "    def reg(self):\n",
    "        return 0.025 * Mix(self)**2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnfrob = FrobClassifier()\n",
    "nnfrob.fit(Xtrain,Ttrain)\n",
    "\n",
    "nnmix = MixClassifier()\n",
    "nnmix.fit(Xtrain,Ttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">    RForest | Train Acc:  1.000 | Test Acc:  0.940\n",
      ">        SVC | Train Acc:  0.958 | Test Acc:  0.951\n",
      ">         NN | Train Acc:  1.000 | Test Acc:  0.884 | Grad:   7.297 | WMix:  40.103 | sqrt(h)*WFrob:  56.739\n",
      ">    NN+Frob | Train Acc:  0.947 | Test Acc:  0.961 | Grad:   0.766 | WMix:   1.732 | sqrt(h)*WFrob:   2.678\n",
      ">     NN+Mix | Train Acc:  0.954 | Test Acc:  0.961 | Grad:   0.749 | WMix:   1.600 | sqrt(h)*WFrob:   4.024\n"
     ]
    }
   ],
   "source": [
    "print(pretty('RForest',rfc))\n",
    "print(pretty('SVC',svc))\n",
    "print(fullpretty('NN',nnc))\n",
    "print(fullpretty('NN+Frob',nnfrob))\n",
    "print(fullpretty('NN+Mix',nnmix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularized neutral networks perform with the baseline of our theorical exercices (see upper / lower bound). The mixed norm penalty reduce the gradient more selectively, and the Frobenius norm take higher Value. Hence we can confirm empirically the the first one \"lower bounds\" the second one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
